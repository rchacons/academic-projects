{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Final MPC \n",
    "## Par Roberto CHACON et Manh-Huan NGUYEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Description du dataset**\n",
    "\n",
    "L'objectif du projet est de prédire le prix des maisons.\n",
    "\n",
    "Le dataset contient 13397 maisons composées des 20 caractéristiques : \n",
    "\n",
    "- **id** - identifiant unique d'une maison\n",
    "- **pricePrice** - la variable cible à predire\n",
    "- **bedroomsNumber** - nb des chambres\n",
    "- **bathroomsNumber** - nb des salles de bain\n",
    "- **sqft_livingsquare** - métrage de la maison\n",
    "- **sqft_lotsquare** - métrage du lot\n",
    "- **floorsTotal** - nb d'étages de la maison\n",
    "- **waterfront** - vue sur le bord de la mer\n",
    "- **view** - Indice de 0 à 4 de la qualité de la vue de la propriété\n",
    "- **condition** - Qualité moyenne de la condition\n",
    "- **grade** - note globale attribué au logement (sur la base du système de notation de King County)\n",
    "- **sqft_above** - Métrage de la maison hors sous-sol\n",
    "- **sqft_basement** - Métrage du sous-sol\n",
    "- **yr_built** - Année de construction\n",
    "- **yr_renovated** - Année de rénovation\n",
    "- **zipcode** - Code postal\n",
    "- **lat** - Coordonnées de latitude\n",
    "- **long** - Coordonnées de longitude\n",
    "- **sqft_living15** - La superficie de l'espace intérieur du logement pour les 15 voisins les plus proches.\n",
    "- **sqft_lot15** - La superficie des terrains des 15 voisins les plus proches.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>3.500</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1860</td>\n",
       "      <td>8378</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1860</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3875</td>\n",
       "      <td>-122.032</td>\n",
       "      <td>1870</td>\n",
       "      <td>8378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17798</th>\n",
       "      <td>5.925</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2170</td>\n",
       "      <td>8240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1370</td>\n",
       "      <td>800</td>\n",
       "      <td>1968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.6291</td>\n",
       "      <td>-122.093</td>\n",
       "      <td>2020</td>\n",
       "      <td>7944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18854</th>\n",
       "      <td>2.555</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1440</td>\n",
       "      <td>43560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1150</td>\n",
       "      <td>290</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.4916</td>\n",
       "      <td>-122.082</td>\n",
       "      <td>1870</td>\n",
       "      <td>56628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13478</th>\n",
       "      <td>13.300</td>\n",
       "      <td>4</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3260</td>\n",
       "      <td>4640</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2360</td>\n",
       "      <td>900</td>\n",
       "      <td>1907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6272</td>\n",
       "      <td>-122.312</td>\n",
       "      <td>3240</td>\n",
       "      <td>5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10509</th>\n",
       "      <td>3.891</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>840</td>\n",
       "      <td>5400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>1948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98118</td>\n",
       "      <td>47.5489</td>\n",
       "      <td>-122.271</td>\n",
       "      <td>1340</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16125</th>\n",
       "      <td>2.900</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>930</td>\n",
       "      <td>7740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>930</td>\n",
       "      <td>0</td>\n",
       "      <td>1924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7091</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1250</td>\n",
       "      <td>7740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19004</th>\n",
       "      <td>3.150</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1730</td>\n",
       "      <td>6368</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1730</td>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3505</td>\n",
       "      <td>-122.032</td>\n",
       "      <td>1780</td>\n",
       "      <td>6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9094</th>\n",
       "      <td>6.850</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3450</td>\n",
       "      <td>8000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2970</td>\n",
       "      <td>480</td>\n",
       "      <td>1927</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5605</td>\n",
       "      <td>-122.402</td>\n",
       "      <td>1880</td>\n",
       "      <td>6135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>3.260</td>\n",
       "      <td>6</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1930</td>\n",
       "      <td>8400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>900</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.4869</td>\n",
       "      <td>-122.340</td>\n",
       "      <td>1780</td>\n",
       "      <td>9520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10054</th>\n",
       "      <td>3.150</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1290</td>\n",
       "      <td>2436</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "      <td>1984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.6803</td>\n",
       "      <td>-122.156</td>\n",
       "      <td>1360</td>\n",
       "      <td>3088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13397 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "6681    3.500         3       2.25         1860      8378     2.0         0.0   \n",
       "17798   5.925         4       3.00         2170      8240     1.0         0.0   \n",
       "18854   2.555         2       1.00         1440     43560     1.0         0.0   \n",
       "13478  13.300         4       2.25         3260      4640     2.0         0.0   \n",
       "10509   3.891         2       1.00          840      5400     1.0         0.0   \n",
       "...       ...       ...        ...          ...       ...     ...         ...   \n",
       "16125   2.900         2       1.00          930      7740     1.0         0.0   \n",
       "19004   3.150         3       2.50         1730      6368     2.0         0.0   \n",
       "9094    6.850         3       2.50         3450      8000     3.0         0.0   \n",
       "3537    3.260         6       1.50         1930      8400     1.0         0.0   \n",
       "10054   3.150         2       2.25         1290      2436     2.0         0.0   \n",
       "\n",
       "       view  condition  grade  sqft_above  sqft_basement  yr_built  \\\n",
       "6681    0.0          3      7        1860              0      1995   \n",
       "17798   0.0          4      8        1370            800      1968   \n",
       "18854   0.0          4      7        1150            290      1965   \n",
       "13478   0.0          5      9        2360            900      1907   \n",
       "10509   0.0          4      7         840              0      1948   \n",
       "...     ...        ...    ...         ...            ...       ...   \n",
       "16125   0.0          3      6         930              0      1924   \n",
       "19004   0.0          3      7        1730              0      1993   \n",
       "9094    0.0          4      8        2970            480      1927   \n",
       "3537    0.0          3      7        1030            900      1971   \n",
       "10054   0.0          3      7        1290              0      1984   \n",
       "\n",
       "       yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "6681            0.0    98038  47.3875 -122.032           1870        8378  \n",
       "17798           0.0    98052  47.6291 -122.093           2020        7944  \n",
       "18854           0.0    98027  47.4916 -122.082           1870       56628  \n",
       "13478           0.0    98112  47.6272 -122.312           3240        5800  \n",
       "10509           0.0    98118  47.5489 -122.271           1340        5400  \n",
       "...             ...      ...      ...      ...            ...         ...  \n",
       "16125           0.0    98125  47.7091 -122.292           1250        7740  \n",
       "19004           0.0    98038  47.3505 -122.032           1780        6597  \n",
       "9094         1975.0    98116  47.5605 -122.402           1880        6135  \n",
       "3537            0.0    98146  47.4869 -122.340           1780        9520  \n",
       "10054           0.0    98052  47.6803 -122.156           1360        3088  \n",
       "\n",
       "[13397 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "houses = pd.read_csv('houses.csv', index_col=0)  # we define the first column of the datafile to be used to index the rows\n",
    "houses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression simple**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette partie est de trouver la meilleure variable pour prédire les prix des maisons.\n",
    "Nous allons donc nous intéresser sur 3 critères : \n",
    "1. Le coefficient de détermination $R^2$.\n",
    "2. La quantité d'information non expliquée par le modèle $I_r$.\n",
    "3. L'erreur de generalisation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La meilleure variable avec le $R^2$ plus élevé et le $I_r$ le plus faible**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 = 0.49640102806105757\n",
      "I_r = -2.376054908381775e-11\n",
      "I_t = 186163.56522629363\n",
      "Meilleure variable = sqft_living\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.320e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Apr 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:44:03</td>     <th>  Log-Likelihood:    </th> <td> -32042.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13397</td>      <th>  AIC:               </th> <td>6.409e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13395</td>      <th>  BIC:               </th> <td>6.410e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>   -0.5466</td> <td>    0.057</td> <td>   -9.646</td> <td> 0.000</td> <td>   -0.658</td> <td>   -0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th> <td>    0.0029</td> <td> 2.49e-05</td> <td>  114.907</td> <td> 0.000</td> <td>    0.003</td> <td>    0.003</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>9718.781</td> <th>  Durbin-Watson:     </th>  <td>   1.981</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>441688.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.993</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>30.485</td>  <th>  Cond. No.          </th>  <td>5.65e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.65e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.496\n",
       "Model:                            OLS   Adj. R-squared:                  0.496\n",
       "Method:                 Least Squares   F-statistic:                 1.320e+04\n",
       "Date:                Fri, 28 Apr 2023   Prob (F-statistic):               0.00\n",
       "Time:                        23:44:03   Log-Likelihood:                -32042.\n",
       "No. Observations:               13397   AIC:                         6.409e+04\n",
       "Df Residuals:                   13395   BIC:                         6.410e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const          -0.5466      0.057     -9.646      0.000      -0.658      -0.436\n",
       "sqft_living     0.0029   2.49e-05    114.907      0.000       0.003       0.003\n",
       "==============================================================================\n",
       "Omnibus:                     9718.781   Durbin-Watson:                   1.981\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           441688.826\n",
       "Skew:                           2.993   Prob(JB):                         0.00\n",
       "Kurtosis:                      30.485   Cond. No.                     5.65e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.65e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared = 0\n",
    "i_r = 0\n",
    "i_t = 0\n",
    "for x in houses.columns[1:] : \n",
    "    X = houses[x]   # On selectionne la colonne contenant la variable prédictive\n",
    "    X = sm.add_constant(X)  # On ajoute une colonne constante pour beta_0\n",
    "    Y = houses['price']  # On selectionne la variable cible\n",
    "    model = sm.OLS(Y,X).fit() # On ajuste le modèle pour prédire Y à l'aide de X\n",
    "    if model.rsquared > r_squared : \n",
    "        r_squared = model.rsquared\n",
    "        i_r = np.sum(model.resid)\n",
    "        i_t = np.sum((houses['price'] - np.mean(houses['price']))**2)\n",
    "        m = model\n",
    "        x_best = x\n",
    "\n",
    "print(f\"R^2 = {r_squared}\")\n",
    "print(f\"I_r = {i_r}\")\n",
    "print(f\"I_t = {i_t}\")\n",
    "print(f\"Meilleure variable = {x_best}\") # La meilleure variable pour predire le prix -> sqft_living\n",
    "m.summary() # Le modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On visualise la relation entre le sqft_living (x) et le prix (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(houses['sqft_living'], houses['price'])\n",
    "plt.title(\"Simple linear regression\")\n",
    "y_pred = m.fittedvalues\n",
    "plt.plot(houses['sqft_living'],y_pred, color =\"red\")\n",
    "plt.xlabel(\"sqft_living\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graph montre une corrélation significative entre le sqft_living et le prix, ce qui confirme que sqft_living est la meilleure variable pour une régression simple pour prédire le prix des maisons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erreur de généralisation**\n",
    "\n",
    "Pour obtenir l'erreur de généralisation, nous allons utiliser la méthode de *Train/Test Split* à l'aide de la librarie sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(houses, test_size= 0.25, random_state=20) # Notre test set sera le 25% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10047, 19) (3350, 19)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)  # On confirme la bonne distribution des données (75%, 25%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faciliter les tests, on utilisera les fonctions implementées en TP pour : \n",
    "- Calculer un modèle de regression sur un dataset.\n",
    "- Calculer une prédiction en fonction d'un modèle.\n",
    "- Estimer l'erreur de généralisation par la méthode train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_regression(data,idx_p,idx_t) :\n",
    "    \"\"\"\n",
    "    Crée un modèle de regression sur un dataset.\n",
    "    :param data: Le dataset.\n",
    "    :param idx_p: Index de la variable predictive.\n",
    "    :param idx_t: Index de la variable à predire.\n",
    "    \"\"\"\n",
    "    X = data.iloc[:,idx_p]\n",
    "    X = sm.add_constant(X)\n",
    "    Y = data.iloc[:,idx_t]\n",
    "    model = sm.OLS(Y,X).fit()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_prediction(my_model,data) :\n",
    "    \"\"\"\n",
    "    Calcule une prédiction sur un dataset en fonction d'un modèle de regression.\n",
    "    :param my_model: Le modèle de regression.\n",
    "    :param data: Le dataset.\n",
    "    \"\"\"\n",
    "    X_names = my_model.model.exog_names[1:]\n",
    "    X_new = data[X_names]\n",
    "    X_new = sm.add_constant(X_new)\n",
    "    pred=my_model.predict(X_new)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization_error_split(train,test, idx_p, idx_t) :\n",
    "    \"\"\" \n",
    "    Calcule l'erreur de généralisation par la méthode train/test split.\n",
    "    :param train: train set\n",
    "    :param test: test set\n",
    "    :param idx_p: variable predictive\n",
    "    :param idx_t: variable à predire\n",
    "    \"\"\" \n",
    "    print()\n",
    "    my_model= my_regression(train,idx_p,idx_t)\n",
    "    pred=my_prediction(my_model,test)\n",
    "    truth = test.iloc[:,idx_t]\n",
    "    MSE=np.mean((pred-truth)**2)\n",
    "    print(MSE)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7.001875041181125\n"
     ]
    }
   ],
   "source": [
    "MSE = generalization_error_split(train,test,houses.columns.get_loc('sqft_living'),houses.columns.get_loc('price'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression multiple**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'effectuer une regression multiple, on utilisera de la même manière qu'avant la fonction OLS, sauf que cette fois ci on mettra les 18 variables du dataset comme input pour predire le prix.\n",
    "\n",
    "Étant donné que nous avons implementé les fonctions realisées en TP, il nous suffira de les appeler pour construire notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.698</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.698</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1820.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 28 Apr 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:44:04</td>     <th>  Log-Likelihood:    </th> <td> -28615.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13397</td>      <th>  AIC:               </th> <td>5.727e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13379</td>      <th>  BIC:               </th> <td>5.740e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    17</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td>   39.0084</td> <td>   38.002</td> <td>    1.026</td> <td> 0.305</td> <td>  -35.481</td> <td>  113.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>      <td>   -0.3653</td> <td>    0.024</td> <td>  -15.152</td> <td> 0.000</td> <td>   -0.413</td> <td>   -0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>     <td>    0.4653</td> <td>    0.042</td> <td>   11.002</td> <td> 0.000</td> <td>    0.382</td> <td>    0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th>   <td>    0.0016</td> <td>    0.000</td> <td>    7.457</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>      <td>-7.137e-07</td> <td> 4.69e-07</td> <td>   -1.522</td> <td> 0.128</td> <td>-1.63e-06</td> <td> 2.06e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors</th>        <td>    0.0513</td> <td>    0.046</td> <td>    1.104</td> <td> 0.270</td> <td>   -0.040</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>    <td>    6.4301</td> <td>    0.235</td> <td>   27.418</td> <td> 0.000</td> <td>    5.970</td> <td>    6.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>view</th>          <td>    0.5149</td> <td>    0.028</td> <td>   18.426</td> <td> 0.000</td> <td>    0.460</td> <td>    0.570</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition</th>     <td>    0.2507</td> <td>    0.030</td> <td>    8.234</td> <td> 0.000</td> <td>    0.191</td> <td>    0.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade</th>         <td>    0.9291</td> <td>    0.028</td> <td>   33.248</td> <td> 0.000</td> <td>    0.874</td> <td>    0.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_above</th>    <td>    0.0004</td> <td>    0.000</td> <td>    1.731</td> <td> 0.083</td> <td> -4.8e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_basement</th> <td> -3.91e-05</td> <td>    0.000</td> <td>   -0.190</td> <td> 0.850</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_built</th>      <td>   -0.0274</td> <td>    0.001</td> <td>  -29.280</td> <td> 0.000</td> <td>   -0.029</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_renovated</th>  <td>    0.0002</td> <td> 4.74e-05</td> <td>    3.714</td> <td> 0.000</td> <td> 8.32e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zipcode</th>       <td>   -0.0057</td> <td>    0.000</td> <td>  -13.222</td> <td> 0.000</td> <td>   -0.006</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat</th>           <td>    6.0461</td> <td>    0.139</td> <td>   43.648</td> <td> 0.000</td> <td>    5.775</td> <td>    6.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long</th>          <td>   -2.2551</td> <td>    0.170</td> <td>  -13.289</td> <td> 0.000</td> <td>   -2.588</td> <td>   -1.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living15</th> <td>    0.0002</td> <td>  4.5e-05</td> <td>    3.541</td> <td> 0.000</td> <td> 7.11e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>11934.573</td> <th>  Durbin-Watson:     </th>  <td>   1.995</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>1421082.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.793</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>52.882</td>   <th>  Cond. No.          </th>  <td>2.14e+08</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.14e+08. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.698\n",
       "Model:                            OLS   Adj. R-squared:                  0.698\n",
       "Method:                 Least Squares   F-statistic:                     1820.\n",
       "Date:                Fri, 28 Apr 2023   Prob (F-statistic):               0.00\n",
       "Time:                        23:44:04   Log-Likelihood:                -28615.\n",
       "No. Observations:               13397   AIC:                         5.727e+04\n",
       "Df Residuals:                   13379   BIC:                         5.740e+04\n",
       "Df Model:                          17                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const            39.0084     38.002      1.026      0.305     -35.481     113.497\n",
       "bedrooms         -0.3653      0.024    -15.152      0.000      -0.413      -0.318\n",
       "bathrooms         0.4653      0.042     11.002      0.000       0.382       0.548\n",
       "sqft_living       0.0016      0.000      7.457      0.000       0.001       0.002\n",
       "sqft_lot      -7.137e-07   4.69e-07     -1.522      0.128   -1.63e-06    2.06e-07\n",
       "floors            0.0513      0.046      1.104      0.270      -0.040       0.142\n",
       "waterfront        6.4301      0.235     27.418      0.000       5.970       6.890\n",
       "view              0.5149      0.028     18.426      0.000       0.460       0.570\n",
       "condition         0.2507      0.030      8.234      0.000       0.191       0.310\n",
       "grade             0.9291      0.028     33.248      0.000       0.874       0.984\n",
       "sqft_above        0.0004      0.000      1.731      0.083    -4.8e-05       0.001\n",
       "sqft_basement  -3.91e-05      0.000     -0.190      0.850      -0.000       0.000\n",
       "yr_built         -0.0274      0.001    -29.280      0.000      -0.029      -0.026\n",
       "yr_renovated      0.0002   4.74e-05      3.714      0.000    8.32e-05       0.000\n",
       "zipcode          -0.0057      0.000    -13.222      0.000      -0.006      -0.005\n",
       "lat               6.0461      0.139     43.648      0.000       5.775       6.318\n",
       "long             -2.2551      0.170    -13.289      0.000      -2.588      -1.922\n",
       "sqft_living15     0.0002    4.5e-05      3.541      0.000    7.11e-05       0.000\n",
       "==============================================================================\n",
       "Omnibus:                    11934.573   Durbin-Watson:                   1.995\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1421082.094\n",
       "Skew:                           3.793   Prob(JB):                         0.00\n",
       "Kurtosis:                      52.882   Cond. No.                     2.14e+08\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.14e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_model = my_regression(houses, range(1,18), 0)\n",
    "multiple_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.448779409174057\n"
     ]
    }
   ],
   "source": [
    "# On obtient les train et test sets\n",
    "train , test = train_test_split(houses, test_size = 0.30, random_state=20)\n",
    "\n",
    "# On calcule le MSE\n",
    "MSE = generalization_error_split(train, test, range(1,18), 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer que le modèle à 18 variables a un MSE plus petit (MSE = 4.44) que celui d'une variable (MSE = 7.001). \n",
    "Donc il est clairement plus intéressant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sélection des variables**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie consiste à trouver le meilleur sous-ensemble de variables parmi les variables predictives ($X_1,X_{18}$) qui conduisent au meilleur modèle.\n",
    "\n",
    "Il existe plusieurs procédures, mais on se centrera sur (En utilisant l'erreur de généralisation comme critère de performance) :\n",
    "- La Recherche Exhaustive.\n",
    "- La Recherche en avant (Forward Search).\n",
    "- La Recherche en arrière (Backward Search).\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Search**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée de la recherche en avant est de partir d'un ensemble vide de variables prédictives, et on itère en ajoutant une à une les variables qui minimisent l'erreur de généralisation (dans notre cas). Le processus continue jusqu'à ce qu'on atteighne un point où l'ajout de variables supplémentaires n'améliore plus le MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(train, idx_p, idx_t):\n",
    "    \"\"\"\n",
    "    Implémente l'algorithme de sélection de variable en avant (forward selection) en utilisant l'erreur de généralisation comme critère de performance\n",
    "    :param train: Le training set.\n",
    "    :param idx_p: Indexes des variables prédictives.\n",
    "    :param idx_t: Index de variable cible.\n",
    "    :return best_variable: La meilleure variable trouvée \n",
    "    \"\"\"\n",
    "    \n",
    "    # D'abord on split le training set en train et test set\n",
    "    train_set, test_set = train_test_split(train, test_size=0.25, random_state=0)\n",
    "    \n",
    "    selected_vars = []\n",
    "    available_vars = np.array(idx_p)\n",
    "\n",
    "    best_var = -1\n",
    "    best_mse = float('inf')\n",
    "    \n",
    "\n",
    "    while len(available_vars) > 0:\n",
    "        \n",
    "        var_changed = False\n",
    "        \n",
    "        # Boucle sur les variables disponibles pour trouver la meilleure variable à ajouter\n",
    "        for var in available_vars:\n",
    "            \n",
    "            current_vars = np.append(selected_vars, var)\n",
    "\n",
    "            # On évalue le MSE en utilisant les variables sélectiionnées jusqu'à l'itération courante\n",
    "            current_mse = generalization_error_split(train,train_set, current_vars, idx_t)\n",
    "\n",
    "            # Si le MSE est amélioré, on maj la variable temporaire\n",
    "            if current_mse < best_mse : \n",
    "                best_var = var\n",
    "                best_mse = current_mse\n",
    "                var_changed = True\n",
    "    \n",
    "        # Si aucune varibale n'a amélioré le MSE, on sort de la boucle\n",
    "        if not var_changed:\n",
    "            break\n",
    "\n",
    "        selected_vars = np.append(selected_vars, best_var)\n",
    "\n",
    "\n",
    "\n",
    "    return best_var, best_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12.951587250280767\n",
      "\n",
      "10.094620077624457\n",
      "\n",
      "7.041470862661589\n",
      "\n",
      "14.017712628397174\n",
      "\n",
      "13.217782131018302\n",
      "\n",
      "13.10361854828772\n",
      "\n",
      "11.983062254786182\n",
      "\n",
      "14.097895771585033\n",
      "\n",
      "7.799913915773282\n",
      "\n",
      "8.915450817933419\n",
      "\n",
      "12.62576131800506\n",
      "\n",
      "14.078179252550107\n",
      "\n",
      "13.979725814137577\n",
      "\n",
      "14.089881675897187\n",
      "\n",
      "12.786335225134868\n",
      "\n",
      "14.112195102260957\n",
      "\n",
      "9.261053761812608\n",
      "\n",
      "6.83859671561452\n",
      "\n",
      "7.041436863966776\n",
      "\n",
      "7.041470862661589\n",
      "\n",
      "7.020192581371728\n",
      "\n",
      "7.041460115863907\n",
      "\n",
      "6.518781358757276\n",
      "\n",
      "6.4609418991897725\n",
      "\n",
      "6.966642360844225\n",
      "\n",
      "6.475590194928873\n",
      "\n",
      "7.031550426344114\n",
      "\n",
      "7.030810815598455\n",
      "\n",
      "6.610873727415732\n",
      "\n",
      "6.964985221095285\n",
      "\n",
      "6.917159373542336\n",
      "\n",
      "6.036858754593451\n",
      "\n",
      "6.714690842324299\n",
      "\n",
      "6.972103484149129\n",
      "\n",
      "5.875699251457948\n",
      "\n",
      "6.036509143960888\n",
      "\n",
      "6.036858754593441\n",
      "\n",
      "6.035520063532472\n",
      "\n",
      "6.03679719752093\n",
      "\n",
      "5.4840116767882074\n",
      "\n",
      "5.449304459690337\n",
      "\n",
      "5.950174045911114\n",
      "\n",
      "5.641149941209308\n",
      "\n",
      "6.0374470261251405\n",
      "\n",
      "6.037581626862971\n",
      "\n",
      "5.784903307675559\n",
      "\n",
      "5.9734868538717025\n",
      "\n",
      "6.031754601072476\n",
      "\n",
      "6.036858754602262\n",
      "\n",
      "5.860810922747679\n",
      "\n",
      "5.9752195221685875\n",
      "\n",
      "5.346643597016542\n",
      "\n",
      "5.44584095388602\n",
      "\n",
      "5.4493044596903175\n",
      "\n",
      "5.4441918323374745\n",
      "\n",
      "5.448199245104604\n",
      "\n",
      "5.219529724924706\n",
      "\n",
      "5.449304459690324\n",
      "\n",
      "5.387410335160691\n",
      "\n",
      "5.098425821553089\n",
      "\n",
      "5.443592160046588\n",
      "\n",
      "5.443805822327713\n",
      "\n",
      "5.301814560603444\n",
      "\n",
      "5.409338789723966\n",
      "\n",
      "5.447204636750297\n",
      "\n",
      "5.4493044596903255\n",
      "\n",
      "5.350161511163666\n",
      "\n",
      "5.418612520729502\n",
      "\n",
      "5.045434533238742\n",
      "\n",
      "5.093446061904469\n",
      "\n",
      "5.0984258215531\n",
      "\n",
      "5.094405352204952\n",
      "\n",
      "5.076199400548883\n",
      "\n",
      "4.855926444607118\n",
      "\n",
      "5.09842582155309\n",
      "\n",
      "4.975049244025197\n",
      "\n",
      "5.0984258215531\n",
      "\n",
      "5.091093018571945\n",
      "\n",
      "5.092191915153224\n",
      "\n",
      "4.672399396344705\n",
      "\n",
      "5.045937750155053\n",
      "\n",
      "5.098220638855533\n",
      "\n",
      "5.098425821553091\n",
      "\n",
      "4.975697730167807\n",
      "\n",
      "5.0995658196866405\n",
      "\n",
      "4.62075996287675\n",
      "\n",
      "4.633358536480572\n",
      "\n",
      "4.672399396344844\n",
      "\n",
      "4.667874175678576\n",
      "\n",
      "4.6644523513524945\n",
      "\n",
      "4.419514331122539\n",
      "\n",
      "4.672399396344985\n",
      "\n",
      "4.6525748204913375\n",
      "\n",
      "4.672399396344622\n",
      "\n",
      "4.672727077724011\n",
      "\n",
      "4.672586648157657\n",
      "\n",
      "4.672399396344799\n",
      "\n",
      "4.668031730033715\n",
      "\n",
      "4.654779352390688\n",
      "\n",
      "4.672399396344791\n",
      "\n",
      "4.652321087964955\n",
      "\n",
      "4.673804979042571\n",
      "\n",
      "4.37997890102971\n",
      "\n",
      "4.3790539176722945\n",
      "\n",
      "4.419514331122791\n",
      "\n",
      "4.416053190361481\n",
      "\n",
      "4.411920695018381\n",
      "\n",
      "4.419514331122873\n",
      "\n",
      "4.419514331122946\n",
      "\n",
      "4.3988719485381065\n",
      "\n",
      "4.4195143311227225\n",
      "\n",
      "4.4199478691886185\n",
      "\n",
      "4.419919075790578\n",
      "\n",
      "4.419514331122678\n",
      "\n",
      "4.417217322564732\n",
      "\n",
      "4.39885763010206\n",
      "\n",
      "4.419514331122779\n",
      "\n",
      "4.40177543652882\n",
      "\n",
      "4.419871649959257\n",
      "\n",
      "4.322046522268276\n",
      "\n",
      "4.379053917672439\n",
      "\n",
      "4.379053917672255\n",
      "\n",
      "4.376993073360949\n",
      "\n",
      "4.37732760946887\n",
      "\n",
      "4.379053917672597\n",
      "\n",
      "4.379053917672469\n",
      "\n",
      "4.3608371992492385\n",
      "\n",
      "4.379053917672419\n",
      "\n",
      "4.378753194027186\n",
      "\n",
      "4.378704596874856\n",
      "\n",
      "4.379053917672531\n",
      "\n",
      "4.378895754722928\n",
      "\n",
      "4.3549305660916\n",
      "\n",
      "4.379053917672302\n",
      "\n",
      "4.365795802488632\n",
      "\n",
      "4.377585809188519\n",
      "\n",
      "4.322046522268473\n",
      "\n",
      "4.322046522268617\n",
      "\n",
      "4.322046522268653\n",
      "\n",
      "4.318368921216308\n",
      "\n",
      "4.320975131673945\n",
      "\n",
      "4.322046522268488\n",
      "\n",
      "4.322046522268482\n",
      "\n",
      "4.298642459729962\n",
      "\n",
      "4.322046522268578\n",
      "\n",
      "4.322370653798343\n",
      "\n",
      "4.322401048874796\n",
      "\n",
      "4.322046522268456\n",
      "\n",
      "4.322277201351902\n",
      "\n",
      "4.2952861615544\n",
      "\n",
      "4.322046522268473\n",
      "\n",
      "4.308690265594903\n",
      "\n",
      "4.3211793925071955\n",
      "\n",
      "4.295286161554525\n",
      "\n",
      "4.295286161554532\n",
      "\n",
      "4.295286161554516\n",
      "\n",
      "4.289483445401217\n",
      "\n",
      "4.292067430569814\n",
      "\n",
      "4.295286161554551\n",
      "\n",
      "4.295286161554511\n",
      "\n",
      "4.277661833938664\n",
      "\n",
      "4.2952861615544276\n",
      "\n",
      "4.296022654904935\n",
      "\n",
      "4.296095704102391\n",
      "\n",
      "4.295286161554588\n",
      "\n",
      "4.295461040181358\n",
      "\n",
      "4.295286161554564\n",
      "\n",
      "4.295286161554491\n",
      "\n",
      "4.246679845509983\n",
      "\n",
      "4.296308053392161\n",
      "\n",
      "4.2466798455099894\n",
      "\n",
      "4.246679845510019\n",
      "\n",
      "4.2466798455101\n",
      "\n",
      "4.2453064363333315\n",
      "\n",
      "4.2434969244239555\n",
      "\n",
      "4.246679845510192\n",
      "\n",
      "4.246679845510017\n",
      "\n",
      "4.230654230523874\n",
      "\n",
      "4.246679845509872\n",
      "\n",
      "4.243969274405984\n",
      "\n",
      "4.243999732325255\n",
      "\n",
      "4.246679845510066\n",
      "\n",
      "4.246500319594924\n",
      "\n",
      "4.246679845509849\n",
      "\n",
      "4.246679845509985\n",
      "\n",
      "4.246679845509821\n",
      "\n",
      "4.244134965381956\n",
      "\n",
      "4.23065423052384\n",
      "\n",
      "4.230654230523654\n",
      "\n",
      "4.230654230523897\n",
      "\n",
      "4.229323834393802\n",
      "\n",
      "4.2255602408150565\n",
      "\n",
      "4.230654230523725\n",
      "\n",
      "4.230654230523749\n",
      "\n",
      "4.230654230523927\n",
      "\n",
      "4.230654230523957\n",
      "\n",
      "4.225605115816213\n",
      "\n",
      "4.225856225052938\n",
      "\n",
      "4.230654230523706\n",
      "\n",
      "4.228895974843594\n",
      "\n",
      "4.230654230523796\n",
      "\n",
      "4.230654230523729\n",
      "\n",
      "4.230654230523916\n",
      "\n",
      "4.227573096903047\n",
      "\n",
      "4.225560240815261\n",
      "\n",
      "4.2255602408153345\n",
      "\n",
      "4.225560240815265\n",
      "\n",
      "4.224502397381441\n",
      "\n",
      "4.225560240815207\n",
      "\n",
      "4.225560240815213\n",
      "\n",
      "4.225560240815295\n",
      "\n",
      "4.2255602408152315\n",
      "\n",
      "4.2255602408153985\n",
      "\n",
      "4.223832704016544\n",
      "\n",
      "4.223996769365838\n",
      "\n",
      "4.225560240815282\n",
      "\n",
      "4.224079971804762\n",
      "\n",
      "4.2255602408151915\n",
      "\n",
      "4.225560240815219\n",
      "\n",
      "4.225560240815171\n",
      "\n",
      "4.221911338843743\n",
      "\n",
      "4.221911338843822\n",
      "\n",
      "4.221911338843763\n",
      "\n",
      "4.221911338843779\n",
      "\n",
      "4.220938993360612\n",
      "\n",
      "4.221911338843751\n",
      "\n",
      "4.221911338843775\n",
      "\n",
      "4.221911338843768\n",
      "\n",
      "4.221911338843765\n",
      "\n",
      "4.221911338843666\n",
      "\n",
      "4.220830366148541\n",
      "\n",
      "4.220929628902141\n",
      "\n",
      "4.2219113388437055\n",
      "\n",
      "4.220026578548276\n",
      "\n",
      "4.221911338843716\n",
      "\n",
      "4.22191133884373\n",
      "\n",
      "4.22191133884387\n",
      "\n",
      "4.221911338843779\n",
      "\n",
      "4.220026578548406\n",
      "\n",
      "4.220026578548297\n",
      "\n",
      "4.220026578548401\n",
      "\n",
      "4.219060804594365\n",
      "\n",
      "4.220026578548145\n",
      "\n",
      "4.220026578548304\n",
      "\n",
      "4.220026578548296\n",
      "\n",
      "4.220026578548341\n",
      "\n",
      "4.220026578548516\n",
      "\n",
      "4.2189224668255\n",
      "\n",
      "4.219047679395448\n",
      "\n",
      "4.220026578548344\n",
      "\n",
      "4.220026578548047\n",
      "\n",
      "4.220026578548374\n",
      "\n",
      "4.22002657854829\n",
      "\n",
      "4.220026578548404\n",
      "\n",
      "4.220026578548384\n",
      "\n",
      "4.218922466825393\n",
      "\n",
      "4.2189224668252905\n",
      "\n",
      "4.218922466825204\n",
      "\n",
      "4.217840151255937\n",
      "\n",
      "4.2189224668253\n",
      "\n",
      "4.218922466825325\n",
      "\n",
      "4.218922466825385\n",
      "\n",
      "4.218922466825548\n",
      "\n",
      "4.2189224668254575\n",
      "\n",
      "4.218922466825415\n",
      "\n",
      "4.2190744841611965\n",
      "\n",
      "4.218922466825474\n",
      "\n",
      "4.218922466825521\n",
      "\n",
      "4.2189224668253305\n",
      "\n",
      "4.218922466825342\n",
      "\n",
      "4.21892246682541\n",
      "\n",
      "4.218922466825296\n",
      "\n",
      "4.2178401512554125\n",
      "\n",
      "4.217840151254883\n",
      "\n",
      "4.217840151256462\n",
      "\n",
      "4.217840151256893\n",
      "\n",
      "4.2178401512553565\n",
      "\n",
      "4.217840151254043\n",
      "\n",
      "4.217840151255263\n",
      "\n",
      "4.217840151255415\n",
      "\n",
      "4.217840151254545\n",
      "\n",
      "4.217840151253006\n",
      "\n",
      "4.217985957052902\n",
      "\n",
      "4.217840151254512\n",
      "\n",
      "4.2178401512539345\n",
      "\n",
      "4.217840151254691\n",
      "\n",
      "4.217840151255047\n",
      "\n",
      "4.21784015125339\n",
      "\n",
      "4.217840151253132\n",
      "\n",
      "4.217840151254731\n",
      "\n",
      "4.217840151254181\n",
      "\n",
      "4.217840151254342\n",
      "\n",
      "4.217840151252472\n",
      "\n",
      "4.2178401512529105\n",
      "\n",
      "4.217840151252102\n",
      "\n",
      "4.217840151254455\n",
      "\n",
      "4.2178401512546255\n",
      "\n",
      "4.217840151254244\n",
      "\n",
      "4.217840151255014\n",
      "\n",
      "4.217985957055525\n",
      "\n",
      "4.217840151255756\n",
      "\n",
      "4.217840151254741\n",
      "\n",
      "4.217840151255995\n",
      "\n",
      "4.217840151253943\n",
      "\n",
      "4.217840151253799\n",
      "\n",
      "4.217840151254744\n",
      "\n",
      "4.21784015125259\n",
      "\n",
      "4.217840151253266\n",
      "\n",
      "4.217840151253105\n",
      "\n",
      "4.217840151253762\n",
      "\n",
      "4.217840151252271\n",
      "\n",
      "4.217840151255045\n",
      "\n",
      "4.217840151252357\n",
      "\n",
      "4.217840151252202\n",
      "\n",
      "4.217840151253762\n",
      "\n",
      "4.217840151255333\n",
      "\n",
      "4.217985957054795\n",
      "\n",
      "4.2178401512532195\n",
      "\n",
      "4.2178401512527115\n",
      "\n",
      "4.217840151255045\n",
      "\n",
      "4.217840151254297\n",
      "\n",
      "4.217840151256376\n",
      "\n",
      "4.217840151255423\n",
      "La meilleure variable est : waterfront(6) avec un MSE de : 4.217840151252102\n"
     ]
    }
   ],
   "source": [
    "# On obtient les train et test sets\n",
    "train , test = train_test_split(houses, test_size = 0.30, random_state=20)\n",
    "\n",
    "best_var, best_mse = forward_selection(train, range(1,18), 0)\n",
    "\n",
    "print(\"La meilleure variable est : \"+houses.columns[best_var]+\"(\"+str(best_var)+\") avec un MSE de : \"+str(best_mse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer que la meilleure variable trouvé est waterfront (index 6) avec un MSE de 4.2178."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Search**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe de la recherche en arrière est le même que celui de la recherche en avant, sauf que cette fois-ci on commence avec un ensemble complet de variables prédictives et on itère en supprimant une à une, jusqu'à ce qu'on atteigne un point où la suppression de variables n'améliore plus le MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection(train, idx_p, idx_t):\n",
    "    \"\"\"\n",
    "    Implémente l'algorithme de sélection de variable en arrière (backward search) en utilisant l'erreur de généralisation comme critère de performance\n",
    "    :param train: Le training set.\n",
    "    :param idx_p: Indexes des variables prédictives.\n",
    "    :param idx_t: Index de variable cible.\n",
    "    :return best_variable: La meilleure variable trouvée \n",
    "    \"\"\"\n",
    "\n",
    "    # D'abord on split le training set en train et test set\n",
    "    train_set, test_set = train_test_split(train, test_size=0.25, random_state=0)\n",
    "\n",
    "    # On commence par selectionner toutes les variables\n",
    "    selected_vars = np.array(idx_p)\n",
    "\n",
    "    removed_vars = []\n",
    "\n",
    "    best_var = -1\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    while len(selected_vars) > 0:\n",
    "\n",
    "        var_changed = False\n",
    "\n",
    "        for var in selected_vars:\n",
    "\n",
    "            current_vars = np.delete(selected_vars, np.where(selected_vars == var))\n",
    "\n",
    "            # On evalue le MSE\n",
    "            current_mse = generalization_error_split(train, train_set, current_vars, idx_t)\n",
    "\n",
    "            if current_mse < best_mse:\n",
    "                best_var = var\n",
    "                best_mse = current_mse\n",
    "                var_changed = True\n",
    "            \n",
    "        \n",
    "        if not var_changed:\n",
    "            break\n",
    "\n",
    "        # On enleve la meilleure variable du selected vars et on l'ajout a la liste des vars supprimees\n",
    "        selected_vars = np.delete(selected_vars, np.where(selected_vars == best_var))\n",
    "        \n",
    "\n",
    "    # On renvoie la dernieree variable supprimee (qui est la meilleure)\n",
    "    return best_var, best_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.2812315800851115\n",
      "\n",
      "4.260350936485808\n",
      "\n",
      "4.238333771531307\n",
      "\n",
      "4.219074484161112\n",
      "\n",
      "4.219997711508929\n",
      "\n",
      "4.457370209919231\n",
      "\n",
      "4.328891953465509\n",
      "\n",
      "4.238980883658211\n",
      "\n",
      "4.609740108780275\n",
      "\n",
      "4.217964582370451\n",
      "\n",
      "4.217840151253607\n",
      "\n",
      "4.4826081924875405\n",
      "\n",
      "4.21987478601242\n",
      "\n",
      "4.2705349239865935\n",
      "\n",
      "4.79548554212836\n",
      "\n",
      "4.267821655430323\n",
      "\n",
      "4.221259276877523\n",
      "\n",
      "4.281159681228139\n",
      "\n",
      "4.260218693440044\n",
      "\n",
      "4.486339637291695\n",
      "\n",
      "4.21892246682546\n",
      "\n",
      "4.219850471998883\n",
      "\n",
      "4.457291243312857\n",
      "\n",
      "4.328713160681686\n",
      "\n",
      "4.238960658704083\n",
      "\n",
      "4.60942116860044\n",
      "\n",
      "4.219060804595446\n",
      "\n",
      "4.4824045533547\n",
      "\n",
      "4.219741745053001\n",
      "\n",
      "4.270407455663686\n",
      "\n",
      "4.7953368576273165\n",
      "\n",
      "4.267674840863281\n",
      "\n",
      "4.221094010103932\n",
      "La meilleure variable est : sqft_basement(11) avec un MSE de : 4.217840151253607\n"
     ]
    }
   ],
   "source": [
    "# On obtient les train et test sets\n",
    "train , test = train_test_split(houses, test_size = 0.30, random_state=20)\n",
    "\n",
    "best_var, best_mse = backward_selection(train, range(1,18), 0)\n",
    "\n",
    "print(\"La meilleure variable est : \"+houses.columns[best_var]+\"(\"+str(best_var)+\") avec un MSE de : \"+str(best_mse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la meilleure variable trouvée avec le backward search est le sqft_basement (d'index 11) avec un MSE de 4.21."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recherche exhaustive**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recherche exhaustive prend en compte toutes les combinaisons possibles de variables prédictives et selectionne celle qui donne les meilleures performances.\n",
    "\n",
    "Pour cette implémentation, on se servira de la librarie *itertools.combinations* pour générer toutes les combinaisons de variables possibles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_search(train, idx_p, idx_t):\n",
    "    \"\"\"\n",
    "    Implémente l'algorithme de sélection de variable par recherche exhaustive en utilisant l'erreur de généralisation comme critère de performance\n",
    "    :param train: Le training set.\n",
    "    :param idx_p: Indexes des variables prédictives.\n",
    "    :param idx_t: Index de variable cible.\n",
    "    :return best_variable: La meilleure variable trouvée \n",
    "    \"\"\"\n",
    "\n",
    "    # D'abord on split le training set en train et test set\n",
    "    train_set, test_set = train_test_split(train, test_size=0.25, random_state=0)\n",
    "\n",
    "    selected_vars = []\n",
    "    available_vars = np.array(idx_p)\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_var = -1\n",
    "\n",
    "    for i in range(len(available_vars)):\n",
    "        for var in itertools.combinations(available_vars, i+1):\n",
    "            \n",
    "            current_vars = list(var)\n",
    "\n",
    "            # On évalue le MSE en utilisant les variables sélectiionnées jusqu'à l'itération courante\n",
    "            current_mse = generalization_error_split(train,train_set, current_vars, idx_t)\n",
    "\n",
    "            # Si le MSE est amélioré, on maj la variable temporaire\n",
    "            if current_mse < best_mse : \n",
    "                best_var = var\n",
    "                best_mse = current_mse\n",
    "            \n",
    "\n",
    "    return best_mse,best_var\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par contre, vu que cette méthode évalue toutes les combinaisons possibles, elle est coûteuse en temps, donc le test ci dessous est commenté pour ne pas bloquer les autres tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# On obtient les train et test sets\\ntrain , test = train_test_split(houses, test_size = 0.30, random_state=20)\\n\\nbest_var = exhaustive_search(train, range(1,18), 0)\\n\\nprint(\"La meilleure variable est : \"+houses.columns[best_var]+\"(\"+str(best_var)+\")\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# On obtient les train et test sets\n",
    "train , test = train_test_split(houses, test_size = 0.30, random_state=20)\n",
    "\n",
    "best_var = exhaustive_search(train, range(1,18), 0)\n",
    "\n",
    "print(\"La meilleure variable est : \"+houses.columns[best_var]+\"(\"+str(best_var)+\")\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modèles non-lineaires**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles non linéaires permettent de modéliser des données plus complexes et ont tendance à être plus précis, mais aussi à être plus difficiles à interpréter que les modèles linéaires.\n",
    "\n",
    "Dans ce TP, nous allons implémenter la technique vue en TP : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_feature(data, idx_p, power):\n",
    "    \"\"\"\n",
    "    Ajoute des variables polynomiales à un dataset donné.\n",
    "\n",
    "    :param data: Le dataset considéré (train, test ou autre)\n",
    "    :param idx_p: La liste des indexes de colonne dans data pour lesquels vous voulez ajouter des variables polynomiales\n",
    "    :param power: La liste des puissances que vous voulez utiliser pour les variables dans idx_p\n",
    "    :return: Un nouveau dataset avec les variables polynomiales ajoutées.\n",
    "    \"\"\"\n",
    "    new_data = data.copy(deep = True)\n",
    "    for i in range(0, len(idx_p)):\n",
    "        for j in power:\n",
    "            for k in range(2, j+1):\n",
    "                new_data['{}_pow_{}'.format(new_data.columns[idx_p[i]],k)] = new_data.iloc[:,idx_p[i]]**k\n",
    "    return(new_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous allons chercher le meilleur degré pour notre modèle en comparant les MSE pour chaque degré : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur degré : 2\n",
      "Meilleur MSE : 3.577486104959523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# On obtient les train et test sets\n",
    "train, test = train_test_split(houses, test_size=0.3, random_state=20)\n",
    "\n",
    "# Index des colonnes à considérer\n",
    "idx_p = range(1,18)\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_degree = 0\n",
    "\n",
    "best_model = LinearRegression()\n",
    "\n",
    "\n",
    "# On boucle sur les degrés de polynômes pour trouver le meilleur degre\n",
    "for degree in range(1, 11):\n",
    "\n",
    "    # Ajout des variables polynomiales au train set\n",
    "    train_poly = add_polynomial_feature(train, idx_p, [degree])\n",
    "    \n",
    "    # On obtient le modele de regression \n",
    "    X_train = train_poly.drop(columns=[\"price\"])\n",
    "    Y_train = train_poly[[\"price\"]]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # On ajout les variables polynomiales au test set\n",
    "    test_poly = add_polynomial_feature(test, idx_p, [degree])\n",
    "    \n",
    "    # Prédiction sur le test set \n",
    "    X_test = test_poly.drop(columns=[\"price\"])\n",
    "    y_test = test_poly[\"price\"]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcul du MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Mise à jour de la meilleure erreur\n",
    "    if mse < best_mse:\n",
    "        best_model = model\n",
    "        best_mse = mse\n",
    "        best_degree = degree\n",
    "\n",
    "# Affichage du meilleur degré et de l'erreur correspondante\n",
    "print(f\"Meilleur degré : {best_degree}\")\n",
    "print(f\"Meilleur MSE : {best_mse}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que, en considèrant les 18 variables, le meilleur degré trouvé est le 2 avec un MSE de 3.57."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle pour Kaggle**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après les tests ci dessus, on peut constater que le meilleur modèle est le non-linéaire, avec un MSE de 3.57. C'est pour cela que nous avons estimer les prédictions de house_competition à partir de ce modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du fichier pour Kaggle\n",
    "houses_competition = pd.read_csv('houses_competition.csv', index_col=0)  \n",
    "\n",
    "houses_competition_poly = add_polynomial_feature(houses_competition, range(0,17), [2])\n",
    "\n",
    "pred = best_model.predict(houses_competition_poly)\n",
    "pred = pred.reshape(-1)\n",
    "\n",
    "pred_df = pd.DataFrame({'ID': houses_competition_poly.index, 'Price' : pred})\n",
    "\n",
    "\n",
    "pred_df.to_csv('my_submission.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score obtenue Kaggle : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./scoreKaggle.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
